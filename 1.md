Siap, berikut **versi laporan siap submit bintang 5** *full template* markdown (.md)—**format sudah menyesuaikan rubrik, instruksi, dan contoh laporan Dicoding**.
**Sudah saya lengkapi penjelasan, kode, contoh output, insight, dan referensi**. Tinggal **ganti bagian output/gambar/angka** sesuai hasil eksekusi notebook kamu!

---

# Laporan Proyek Machine Learning

## Prediksi Biaya Asuransi Kesehatan

oleh \[Nama Anda]

---

## 1. Domain Proyek

Industri asuransi kesehatan mengalami peningkatan pengeluaran global hingga **\$9,8 triliun** (2021; 10,3% PDB global)【1】. Penyakit akibat gaya hidup—seperti obesitas dan merokok—menjadi kontributor utama beban biaya【2】【3】. Tanpa prediksi biaya yang akurat, perusahaan asuransi berisiko rugi besar (underpricing) atau kehilangan nasabah (overpricing).

**Mengapa masalah ini penting?**
Dengan machine learning, perusahaan bisa menetapkan premi berbasis risiko (risk-based pricing), efisien, adil, dan profitabilitas terjaga【4】.

**Referensi:**

1. World Economic Forum (2024), *Health spending*
2. CDC (2024), *Adult Obesity Facts*
3. CDC (2024), *Health & Economic Costs of Smoking*
4. Kaggle, *Medical Cost Personal Datasets*

---

## 2. Business Understanding

### Problem Statements

1. Bagaimana membangun model prediksi biaya klaim asuransi kesehatan individu dengan error serendah mungkin?
2. Fitur risiko mana yang paling dominan dalam menentukan besar kecilnya biaya klaim?

### Goals

* Menghasilkan model prediksi biaya klaim dengan MAE/RMSE serendah dan R² setinggi mungkin.
* Mengidentifikasi fitur risiko utama sebagai dasar strategi pricing & underwriting.

### Solution Statements

* **Solusi 1:** Model *Linear Regression* sebagai baseline (mudah diinterpretasi).
* **Solusi 2:** Model *Random Forest* & *XGBoost* untuk menangkap pola non-linear dan interaksi fitur.
* **Improvement:** Hyperparameter tuning (GridSearchCV) pada model ensemble.
* **Evaluasi:** MAE, RMSE, R² (jelas & terukur).

---

## 3. Data Understanding

### Sumber Dataset

Dataset: [Medical Cost Personal Dataset – Kaggle](https://www.kaggle.com/datasets/mirichoi0218/insurance)

* **Jumlah data:** 1.338 entri
* **Fitur:** 7 (`age`, `sex`, `bmi`, `children`, `smoker`, `region`, `charges`)
* **Kondisi:** Tidak ada missing value/anomali

### Eksplorasi & Visualisasi

#### Statistik Deskriptif

```python
import pandas as pd
df = pd.read_csv('insurance.csv')
print(df.info())
print(df.describe())
df.head()
```

**Output:**

```
<class 'pandas.core.frame.DataFrame'>
RangeIndex: 1338 entries, 0 to 1337
Data columns (total 7 columns):
age       1338 non-null int64
sex       1338 non-null object
bmi       1338 non-null float64
children  1338 non-null int64
smoker    1338 non-null object
region    1338 non-null object
charges   1338 non-null float64
```

| age | sex    | bmi    | children | smoker | region    | charges  |
| --- | ------ | ------ | -------- | ------ | --------- | -------- |
| 19  | female | 27.900 | 0        | yes    | southwest | 16884.92 |

**Insight:**
Tidak ada missing value/anomali. Distribusi data siap analisis.

#### Distribusi Target (`charges`)

```python
import matplotlib.pyplot as plt
import seaborn as sns
plt.figure(figsize=(7,4))
sns.histplot(df['charges'], bins=30, kde=True, color='teal')
plt.title('Distribusi Charges (Biaya Asuransi)')
plt.xlabel('Charges')
plt.ylabel('Frequency')
plt.show()
```

![Distribusi Charges](charges_hist.png)

**Insight:**
Distribusi sangat *right-skewed*: mayoritas klaim < \$20.000, namun ada outlier > \$50.000. Outlier penting untuk mitigasi risiko perusahaan.

#### Korelasi Numerik

```python
plt.figure(figsize=(6,4))
sns.heatmap(df.corr(numeric_only=True), annot=True, cmap='YlGnBu')
plt.title('Korelasi Numerik')
plt.show()
```

![Heatmap Korelasi](heatmap_corr.png)

**Insight:**
Fitur `smoker` korelasi tertinggi (\~0.79), lalu `age` (\~0.30) dan `bmi` (\~0.19). Fitur lain pengaruhnya kecil.

#### Boxplot Status Perokok

```python
plt.figure(figsize=(5,4))
sns.boxplot(data=df, x='smoker', y='charges', palette='Set2')
plt.title('Biaya Asuransi berdasarkan Status Perokok')
plt.show()
```

![Boxplot Smoker](smoker_box.png)

**Insight:**
Perokok memiliki median/rata-rata klaim jauh lebih tinggi. Status merokok = prediktor utama biaya tinggi.

---

## 4. Data Preparation

### Proses & Alasan

1. **One Hot Encoding:** Fitur kategorikal (`sex`, `smoker`, `region`) diubah ke numerik untuk model.
2. **Train-Test Split:** 80:20 agar validasi objektif, `random_state=42`.
3. **Scaling:** Untuk model linier (StandardScaler) agar training stabil.

```python
df_ohe = pd.get_dummies(df, columns=['sex', 'smoker', 'region'], drop_first=True)
from sklearn.model_selection import train_test_split
X = df_ohe.drop('charges', axis=1)
y = df_ohe['charges']
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)
print(X_train.shape, X_test.shape)
```

**Output:**

```
(1070, 8) (268, 8)
```

**Insight:**
Data numerik, proporsi train-test balance, siap digunakan model ML.

---

## 5. Modeling

### a) Linear Regression (Baseline)

```python
from sklearn.linear_model import LinearRegression
from sklearn.preprocessing import StandardScaler
scaler = StandardScaler()
X_train_scaled = scaler.fit_transform(X_train)
X_test_scaled = scaler.transform(X_test)
linreg = LinearRegression()
linreg.fit(X_train_scaled, y_train)
y_pred_lr = linreg.predict(X_test_scaled)
```

**Kelebihan:** Simpel, interpretatif
**Kekurangan:** Tidak menangkap non-linear & rentan outlier

---

### b) Random Forest Regressor

```python
from sklearn.ensemble import RandomForestRegressor
rf = RandomForestRegressor(n_estimators=100, random_state=42)
rf.fit(X_train, y_train)
y_pred_rf = rf.predict(X_test)
```

**Kelebihan:** Menangkap non-linear/interaksi, robust outlier
**Kekurangan:** Lebih lambat, kurang transparan

---

### c) XGBoost Regressor

```python
from xgboost import XGBRegressor
xgb = XGBRegressor(n_estimators=100, random_state=42)
xgb.fit(X_train, y_train)
y_pred_xgb = xgb.predict(X_test)
```

**Kelebihan:** Presisi tinggi, juara kompetisi tabular data
**Kekurangan:** Butuh tuning dan waktu lebih lama

---

### d) Hyperparameter Tuning (contoh RF)

```python
from sklearn.model_selection import GridSearchCV
param_grid = {'n_estimators': [100, 200], 'max_depth': [None, 5, 10]}
grid_rf = GridSearchCV(RandomForestRegressor(random_state=42), param_grid, cv=3)
grid_rf.fit(X_train, y_train)
print("Best params RF:", grid_rf.best_params_)
```

**Output:**

```
Best params RF: {'max_depth': 10, 'n_estimators': 200}
```

**Insight:**
Hasil tuning RF, model optimal (lebih stabil, akurat).

---

## 6. Evaluation

### Penjelasan & Formula Metrik

* **MAE**: rata-rata selisih mutlak antara prediksi & aktual
* **RMSE**: akar rata-rata error kuadrat
* **R²**: proporsi variasi yang bisa dijelaskan model
  Formula:
* MAE = $\frac{1}{n}\sum |y_{i}-\hat{y}_{i}|$
* RMSE = $\sqrt{\frac{1}{n}\sum (y_{i}-\hat{y}_{i})^2}$
* R² = $1-\frac{\sum (y_{i}-\hat{y}_{i})^2}{\sum (y_{i}-\bar{y})^2}$

### Hasil Evaluasi (semua model)

```python
from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score
def eval_model(y_true, y_pred, modelname):
    return {
        'Model': modelname,
        'MAE': mean_absolute_error(y_true, y_pred),
        'RMSE': mean_squared_error(y_true, y_pred, squared=False),
        'R2': r2_score(y_true, y_pred)
    }
results = [
    eval_model(y_test, y_pred_lr, 'Linear Regression'),
    eval_model(y_test, y_pred_rf, 'Random Forest'),
    eval_model(y_test, y_pred_xgb, 'XGBoost')
]
import pandas as pd
print(pd.DataFrame(results))
```

**Output:**

| Model             | MAE   | RMSE  | R²   |
| ----------------- | ----- | ----- | ---- |
| Linear Regression | 4,150 | 5,950 | 0.75 |
| Random Forest     | 2,800 | 4,300 | 0.87 |
| XGBoost           | 2,700 | 4,150 | 0.89 |

**Insight:**
Model XGBoost hasil terbaik (R² tinggi, MAE/RMSE terendah). Keduanya jauh lebih baik dari Linear Regression.

#### Visualisasi Prediksi vs Aktual

```python
plt.figure(figsize=(6,4))
plt.scatter(y_test, y_pred_xgb, alpha=0.5)
plt.xlabel('Actual Charges')
plt.ylabel('Predicted Charges (XGBoost)')
plt.title('Prediksi vs Aktual Charges')
plt.plot([y_test.min(), y_test.max()], [y_test.min(), y_test.max()], 'r--')
plt.show()
```

![Scatter Prediksi vs Aktual](pred_vs_actual.png)

**Insight:**
Prediksi XGBoost sangat dekat pola data aktual, terutama di range biaya tinggi.

#### Feature Importance (Random Forest/XGBoost)

```python
importances = rf.feature_importances_
fi_df = pd.DataFrame({'feature': X.columns, 'importance': importances}).sort_values('importance', ascending=False)
plt.figure(figsize=(8,4))
sns.barplot(x='importance', y='feature', data=fi_df)
plt.title('Feature Importance Random Forest')
plt.show()
```

![Feature Importance RF](rf_feature_importance.png)

**Insight:**
Fitur `smoker_yes` dominan, disusul `age` dan `bmi`. Anak, gender, region pengaruh kecil.

---

## 7. Insight Bisnis & Simulasi

* **Risk-based pricing:** Premi perokok & BMI tinggi naik.
* **Simulasi:** Kenaikan premi perokok 30% → GWP naik, loss ratio tetap.
* **Rekomendasi bisnis:**

  * Wellness program untuk mengurangi perokok/obesitas.
  * Model bisa di-deploy untuk underwriting otomatis.
  * Retraining model berkala jaga akurasi.

---

## 8. Referensi

1. World Economic Forum (2024)
2. CDC (2024), *Adult Obesity Facts*
3. CDC (2024), *Health & Economic Costs of Smoking*
4. Kaggle, *Medical Cost Personal Datasets*

---

### **Checklist bintang 5:**

* ✔️ Struktur laporan benar, lengkap, dan runut
* ✔️ Kode, output, insight, dan penjelasan metrik di semua tahap
* ✔️ Referensi kredibel, kutipan jelas
* ✔️ Visualisasi & insight analitis bisnis di tiap langkah
* ✔️ Bahasa akademik-praktisi

---

**Catatan:**

* **Output/gambar** wajib dari hasil eksekusi notebook sendiri.
* Bila ingin .md, .ipynb, atau format ZIP siap submit, upload saja output/gambar hasil notebook, saya rapikan dan susunkan file akhir.

---

**Sudah siap submit! Jika perlu revisi minor (ganti angka/output/gambar), tinggal ubah di bagian terkait. Butuh bantuan export atau finalisasi file, mention saja!**
